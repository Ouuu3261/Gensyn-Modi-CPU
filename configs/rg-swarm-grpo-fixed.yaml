# GRPO兼容的配置文件
training:
  fp16: false
  bf16: false
  dataloader_pin_memory: false
  remove_unused_columns: false
  
trainer:
  config:
    _target_: trl.trainer.GRPOConfig
    fp16: false
    bf16: false
    torch_dtype: null  # 让transformers自动选择
    dataloader_pin_memory: false
    remove_unused_columns: false
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 1
    learning_rate: 1e-5
    num_train_epochs: 1
    logging_steps: 10
    save_steps: 100
    eval_steps: 100
    warmup_steps: 0
    
models:
  _target_: transformers.AutoModelForCausalLM.from_pretrained
  pretrained_model_name_or_path: microsoft/DialoGPT-small
  torch_dtype: "torch.float32"  # 强制使用float32
  device_map: null
  trust_remote_code: false
  
tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: microsoft/DialoGPT-small
  trust_remote_code: false
  padding_side: left
